{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef36635b-b0fe-4a0f-a260-b01de559421b",
   "metadata": {
    "id": "ef36635b-b0fe-4a0f-a260-b01de559421b"
   },
   "source": [
    "# ACTION RECOGINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2e673-6c17-49e3-aaa7-f6ead4a74255",
   "metadata": {
    "id": "33e2e673-6c17-49e3-aaa7-f6ead4a74255"
   },
   "source": [
    "### Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718e4f0-323b-475c-9a26-7e3e16d1e95f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0718e4f0-323b-475c-9a26-7e3e16d1e95f",
    "outputId": "85e6ed9e-8e8e-4844-b403-595d6aea911b"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127953c2-f7b2-4aff-8af4-661ffbea57a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "127953c2-f7b2-4aff-8af4-661ffbea57a0",
    "outputId": "8bb5351c-1024-4b6a-c913-33085462fa13"
   },
   "outputs": [],
   "source": [
    "import cv2      #video capture\n",
    "import numpy as np    #data manuplation\n",
    "import os    #easier file paths\n",
    "from matplotlib import pyplot as pltb    #for stats graphs\n",
    "import time    #sleeps between frames we collect\n",
    "import mediapipe as mp    #key points of face, arm etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6c72a-22a3-4d63-883e-71a3b94ac692",
   "metadata": {
    "id": "6ba6c72a-22a3-4d63-883e-71a3b94ac692"
   },
   "source": [
    "### Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aa5f1a-2bb0-446b-b919-be6cf30275af",
   "metadata": {
    "id": "74aa5f1a-2bb0-446b-b919-be6cf30275af"
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic     #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils    #drawing utilities\n",
    "mp_face = mp.solutions.face_mesh    #for face connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafdec0f-3c28-4aa3-8ee8-45eb66047e08",
   "metadata": {
    "id": "cafdec0f-3c28-4aa3-8ee8-45eb66047e08"
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    #BGR -> RGB\n",
    "    image.flags.writeable = False     #image no longer writable\n",
    "    results = model.process(image)     #make prediction\n",
    "    image.flags.writeable = True     #image is now writable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)    #RGB -> BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a294d4b-d0c3-44b2-b4c5-811ab6d6fd10",
   "metadata": {
    "id": "3a294d4b-d0c3-44b2-b4c5-811ab6d6fd10"
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #  draws landmarks easily        the landmarks           connection b/w landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_face.FACEMESH_TESSELATION)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28a934a-9d77-4b47-9432-fb854a5572a4",
   "metadata": {
    "id": "e28a934a-9d77-4b47-9432-fb854a5572a4"
   },
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_face.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color = (80,110,10), thickness = 1,circle_radius = 1),   # landmark\n",
    "                              mp_drawing.DrawingSpec(color = (80,356,121), thickness = 1,circle_radius = 1))   # connection\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color = (80,110,10), thickness = 1,circle_radius = 1),   # landmark\n",
    "                             mp_drawing.DrawingSpec(color = (80,356,121), thickness = 1,circle_radius = 1))   # connection\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color = (80,110,10), thickness = 1,circle_radius = 1),   # landmark\n",
    "                              mp_drawing.DrawingSpec(color = (80,356,121), thickness = 1,circle_radius = 1))   # connection\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color = (80,110,10), thickness = 1,circle_radius = 1),   # landmark\n",
    "                              mp_drawing.DrawingSpec(color = (80,356,121), thickness = 1,circle_radius = 1))   # connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fmOXzr49-jn",
   "metadata": {
    "id": "1fmOXzr49-jn"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76151c-555c-40ac-8d07-d7b2fe6cc000",
   "metadata": {
    "id": "3d76151c-555c-40ac-8d07-d7b2fe6cc000"
   },
   "outputs": [],
   "source": [
    "#VIDEO CAPTURE\n",
    "cam = cv2.VideoCapture(0)   #access web cam\n",
    "#mediapipe model          intial detection                tracking further from detection\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cam.isOpened():    #checks if our web cam working\n",
    "        ret, frame = cam.read()    #read feed (return val, frame)\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)     #make detection\n",
    "        # print(results)\n",
    "\n",
    "        draw_style_landmarks(image, results)    #draw landmarks\n",
    "\n",
    "        cv2_imshow('OpenCV Feed', image)   #show to screen\n",
    "\n",
    "        #breaking gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):    #if key == q we break our loop\n",
    "            break\n",
    "cam.release()    #break the web cam\n",
    "# cv2.destroyAllWindows()     #deletes all the frmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80614eb-c9b4-4737-ad44-f41e44a294ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "e80614eb-c9b4-4737-ad44-f41e44a294ec",
    "outputId": "a12a3de8-0f4b-47ac-b9e4-5628ab089156",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.left_hand_landmarks.landmark   #list\n",
    "# count\n",
    "# face_landmarks\n",
    "# index\n",
    "# left_hand_landmarks\n",
    "# mro\n",
    "# pose_landmarks\n",
    "# right_hand_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a3382-0472-48ac-80c1-004165f6958e",
   "metadata": {
    "id": "896a3382-0472-48ac-80c1-004165f6958e"
   },
   "source": [
    "### Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d528414-d043-4cd9-b2e6-c1e3caf9a494",
   "metadata": {
    "id": "3d528414-d043-4cd9-b2e6-c1e3caf9a494"
   },
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92ac51-6997-463d-981e-1f8dfba8fbbb",
   "metadata": {
    "id": "6c92ac51-6997-463d-981e-1f8dfba8fbbb"
   },
   "source": [
    "### Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f824fd-7c38-4e6e-9ba1-a9790e27bd9b",
   "metadata": {
    "id": "b2f824fd-7c38-4e6e-9ba1-a9790e27bd9b"
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = os.path.join('MP_Data')    #path of exported data np arrays\n",
    "DATA_PATH = '/content/MP_Data'\n",
    "actions = np.array(['Hello', 'Thanks', 'I_love_you', 'Yes', 'No', 'Please', 'Sorry', 'Help',\n",
    "                    'You', 'Me', 'Stop'])   #actions we need to detect\n",
    "no_sequence = 30    #30 videos of data\n",
    "sequence_length = 30    #length of videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c9806-1990-45f6-a926-46f08d4b84bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "386c9806-1990-45f6-a926-46f08d4b84bc",
    "outputId": "dd6c1113-c851-483a-87ab-6fbdbc7ee6bb"
   },
   "outputs": [],
   "source": [
    "#makes folder for evry action that contains 30 video folders\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequence):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fe8d7-537b-4832-aa06-dc52499917da",
   "metadata": {
    "id": "b39fe8d7-537b-4832-aa06-dc52499917da"
   },
   "source": [
    "### Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea096c-b0e0-4dc7-a40b-c589d5a295fe",
   "metadata": {
    "id": "47ea096c-b0e0-4dc7-a40b-c589d5a295fe"
   },
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)   #access web cam\n",
    "#mediapipe model          intial detection                tracking further from detection\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    for action in actions:    #loops through action\n",
    "        for sequence in range(no_sequence):    #loop thro seq aka videos\n",
    "            for frame_num in range(sequence_length):    #loops thro each frame aka video lenth\n",
    "                ret, frame = cam.read()    #read feed (return val, frame)\n",
    "                image, results = mediapipe_detection(frame, holistic)     #make detection\n",
    "                # print(results)\n",
    "                draw_style_landmarks(image, results)    #draw landmarks\n",
    "\n",
    "                #break\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (50, 100),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image,  'Collecting frames for {} Video number {}'.format(action, sequence),\n",
    "                                (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(2500)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collectingframes for {} Video number {}'.format(action, sequence),\n",
    "                                (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                keypoints = extract_keypoints(results)    #extrats keypoints from result\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))   #frame name, where we save our frame\n",
    "                np.save(npy_path, keypoints)    #saves keypoints at the path\n",
    "\n",
    "                cv2.imshow('OpenCV Feed', image)   #show to screen\n",
    "                #breaking gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):    #if key == q we break our loop\n",
    "                    break\n",
    "cam.release()    #break the web cam\n",
    "cv2.destroyAllWindows()     #deletes all the frmaes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c07d00-0974-4636-8da1-816132275342",
   "metadata": {
    "id": "14c07d00-0974-4636-8da1-816132275342"
   },
   "source": [
    "### Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c7a614-bcfe-4e36-9e21-2dea8e049cf7",
   "metadata": {
    "id": "d1c7a614-bcfe-4e36-9e21-2dea8e049cf7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    #for splitting data for test and train\n",
    "from tensorflow.keras.utils import to_categorical    #to convert to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69065e-59b1-42c5-b0dd-5ed61ee3e024",
   "metadata": {
    "id": "0e69065e-59b1-42c5-b0dd-5ed61ee3e024"
   },
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}    #creates map for actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3928a1a-8c4d-426f-92ee-a4884f41a604",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3928a1a-8c4d-426f-92ee-a4884f41a604",
    "outputId": "ba4f3d0a-b3f8-4df1-efcb-ab6269152036"
   },
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lexh2HVbGm_9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lexh2HVbGm_9",
    "outputId": "27878d11-f009-4675-dc3d-58a2909073e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da9bc9-1ee4-46ed-b643-8a471f0c182d",
   "metadata": {
    "id": "14da9bc9-1ee4-46ed-b643-8a471f0c182d"
   },
   "outputs": [],
   "source": [
    "sequences, labels = [], []     #videos with there action\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequence):\n",
    "        window = []     #one video of a action\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            window.append(res)     #one frame added each time\n",
    "        sequences.append(window)      #one video appended\n",
    "        labels.append(label_map[action])    #for that video it's label(action) is appended\n",
    "\n",
    "X = np.array(sequences)     #array of all videos\n",
    "y = to_categorical(labels).astype(int)     #one-hot encoding\n",
    "\n",
    "np.savez_compressed(\"X_y_data.npz\", X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZbWkm3MWfSfF",
   "metadata": {
    "id": "ZbWkm3MWfSfF"
   },
   "outputs": [],
   "source": [
    "data = np.load(\"X_y_data.npz\")\n",
    "X = data[\"X\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220e457-8bf6-49c3-9025-6341c5401673",
   "metadata": {
    "id": "3220e457-8bf6-49c3-9025-6341c5401673"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c191c17-a50f-4cd0-b7aa-1937a0145341",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c191c17-a50f-4cd0-b7aa-1937a0145341",
    "outputId": "08c1fad9-6176-4dda-e0e6-8399a76eec19"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H3UcPP0boHoT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3UcPP0boHoT",
    "outputId": "119f94ca-6165-49f6-b7b5-45eaf23b9b7f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
    "print(X_train_tensor.shape)\n",
    "print(X_train_tensor)\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "print(test_dataset)\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bc920-1f68-4f25-8847-49a361a52302",
   "metadata": {
    "id": "921bc920-1f68-4f25-8847-49a361a52302"
   },
   "source": [
    "### Build and Train Encoder Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a8ca1f-a741-4dc9-b3f2-4ab175393e73",
   "metadata": {
    "id": "58a8ca1f-a741-4dc9-b3f2-4ab175393e73"
   },
   "outputs": [],
   "source": [
    "import torch     #core pytorch lib\n",
    "import torch.nn as nn     #layers like linear, transformer, ReLu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb3e435-9296-4300-bfaf-69106ca120d6",
   "metadata": {
    "id": "aeb3e435-9296-4300-bfaf-69106ca120d6"
   },
   "outputs": [],
   "source": [
    "#                             calls all base class of all models\n",
    "class SignLanguageTransformer(nn.Module):    #a nn class for our project\n",
    "#                     (features pre frame)             (no of actions)           (no of attention heads)       (size of ff net in each encoder)\n",
    "    def __init__(self, input_dim = 1662, seq_len = 30, num_classes = 11, d_model = 512, nhead = 4, num_layers = 2, dim_feedforward = 1024, dropout = 0.1):\n",
    "#                                  (no of frames in one seq)   (size of each embedding vector)     (no of layers)                       (d rate for reglarization)\n",
    "        super(SignLanguageTransformer, self).__init__()    #to register layers with PyTorch\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)    #linear layer projecting i/p features (fame -> low dimensional space)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))   #learnable pos embedding\n",
    "        encoder_layer = nn.TransformerEncoderLayer(    #create encoderlayer\n",
    "            d_model = d_model,    #i/p \\ o/p vectors\n",
    "            nhead = nhead,     #no od attention heads to split each vector\n",
    "            dim_feedforward = dim_feedforward,    #size of internal MLP(ff block)\n",
    "            dropout = dropout,    #for regularization\n",
    "            batch_first = True     #ensure shape(batch, seq_len, features)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)     #stacks multiple encoder layers\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)   #when  transformer o/p a seq of 30 frame embeddings, avgs to single vector\n",
    "        self.classifier = nn.Sequential(    #final classifier\n",
    "            nn.Linear(d_model, 256),     #projects t o/p 512 -> 256 -> 11 (actions)\n",
    "            nn.ReLU(),    #better learning\n",
    "            nn.Dropout(dropout),    #better generization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):    #how i/p passes through network\n",
    "        x = self.input_proj(x) + self.pos_embedding    #project i/p features (512 -> 256)\n",
    "        # x -> (batch, 30, 512)\n",
    "        x = self.transformer(x)   #feeds seq thro trans stack(allows self attention to learn rel b/w frames)\n",
    "        x = x.permute(0, 2, 1)    #(batch, features, seq_len) as AdaptiveAvgPool1d expects features on axis 1.\n",
    "        x = self.global_pool(x).squeeze(2)    #o/p -> (batch, 512)\n",
    "        return self.classifier(x)    #(batch, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82971f25-64e6-44aa-b79f-ee53b5e46a83",
   "metadata": {
    "id": "82971f25-64e6-44aa-b79f-ee53b5e46a83"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F     #imports the funtional API of nn lib\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    #checks if GPU avail otherwisw sets cpu\n",
    "model = SignLanguageTransformer().to(device)    #moves weights to the device selected\n",
    "criterion = nn.CrossEntropyLoss()   #sets the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)   #uses Adam and tells it to update all weights\n",
    "                                       #learning rate = 0.0001    (Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qV1HKj6ioJBf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV1HKj6ioJBf",
    "outputId": "b174178f-0122-42bb-c207-a036f5eff2f5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e0803-63c8-4dc7-8af6-c716090635e1",
   "metadata": {
    "id": "6b3e0803-63c8-4dc7-8af6-c716090635e1"
   },
   "source": [
    "### Make Predictions & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da74ee9-6b89-4425-8f11-48844f5d454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())  # fixed line\n",
    "\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=actions, yticklabels=actions)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a2b00-3dbf-4d9b-a877-c7408bff0c49",
   "metadata": {
    "id": "f75a2b00-3dbf-4d9b-a877-c7408bff0c49"
   },
   "source": [
    "### Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "igGo43o85dY8",
   "metadata": {
    "id": "igGo43o85dY8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignLanguageTransformer(\n",
       "  (input_proj): Linear(in_features=1662, out_features=512, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), 'sign_lang_model_weights_100.00_final.pth')\n",
    "\n",
    "model = SignLanguageTransformer().to(device)\n",
    "model.load_state_dict(torch.load('sign_lang_model_weights_94.12.pth', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7d1e7-5ef7-42d0-9f9b-7cd5367f85a6",
   "metadata": {
    "id": "31a7d1e7-5ef7-42d0-9f9b-7cd5367f85a6"
   },
   "outputs": [],
   "source": [
    "# !unzip /content/MP_Data.zip -d /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a12701-85ab-46e3-8a28-8536e0617102",
   "metadata": {
    "id": "38a12701-85ab-46e3-8a28-8536e0617102"
   },
   "source": [
    "### Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pSCxdllN7GTT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "pSCxdllN7GTT",
    "outputId": "7ac4eefb-689d-4417-d1a7-0e562a363f46"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "colors = [\n",
    "    (245, 117, 16),   # Orange\n",
    "    (117, 245, 16),   # Lime\n",
    "    (16, 117, 245),   # Blue\n",
    "    (255, 0, 0),      # Red\n",
    "    (0, 255, 0),      # Green\n",
    "    (0, 0, 255),      # Dark Blue\n",
    "    (255, 255, 0),    # Yellow\n",
    "    (255, 0, 255),    # Magenta\n",
    "    (0, 255, 255),    # Cyan\n",
    "    (128, 0, 128),    # Purple\n",
    "    (255, 165, 0)     # Dark Orange\n",
    "]\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "AruN26yA975W",
   "metadata": {
    "id": "AruN26yA975W"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from scipy import stats\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Real-time detection loop\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_time = time.time()\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect landmarks\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Extract keypoints & build sequence\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "\n",
    "        # Predict when we have enough frames\n",
    "        if len(sequence) == 30:\n",
    "            input_tensor = torch.tensor(np.expand_dims(sequence, axis=0), dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                res = model(input_tensor).detach().cpu().numpy()[0]\n",
    "\n",
    "            predicted_class = np.argmax(res)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "            # Debouncing logic: only accept prediction if consistent for 10 frames\n",
    "            if np.unique(predictions[-10:])[0] == predicted_class:\n",
    "                if res[predicted_class] > threshold:\n",
    "                    if len(sentence) == 0 or actions[predicted_class] != sentence[-1]:\n",
    "                        sentence.append(actions[predicted_class])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "        else:\n",
    "            res = np.zeros(len(actions))  # For visualization before 30 frames\n",
    "\n",
    "        # Visualize predictions\n",
    "        image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "        # Display sentence\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # FPS (optional)\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "        cv2.putText(image, f'FPS: {int(fps)}', (500, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        # Show final frame\n",
    "        cv2.imshow('Sign Language Detection', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75f65a-7120-46aa-8927-762bf2cce89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
